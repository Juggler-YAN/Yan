# 强化学习

智能体**感知**环境状态，**决策**采取动作，环境根据智能体动作生成**奖励**并产生新的状态，然后再开始下一轮。

![RL](./img/rl.png "RL")

## 1.基本概念

强化学习的数学描述——Markov决策过程

1. Markov过程（MP） $<\mathcal{S},\mathcal{P}>$

- $\mathcal{S}$ 是状态集合，$\mathcal{S} = \{s_1, s_2, ..., s_n\} $
- $\mathcal{P}$ 是状态转移矩阵，

$$
\mathcal{P} = 
\begin{bmatrix}
P(s_1|s_1) & \cdots & P(s_n|s_1) \\
\vdots & \ddots & \vdots \\
P(s_1|s_n) & \cdots & P(s_n|s_n) \\
\end{bmatrix}
$$

2.  Markov奖励过程（MRP） $<\mathcal{S},\mathcal{P},r,\gamma>$

- $\mathcal{S}$ 是状态集合
- $\mathcal{P}$ 是状态转移矩阵
- $r(s)$ 是奖励函数，某个状态 $s$ 的奖励 $r(s)$ 转移到各状态时可以获得的奖励的期望
- $\gamma$ 是折扣因子，取值范围为[0,1)，接近1更关注长期奖励，接近0更关注短期奖励

3.  Markov决策过程（MDP） $<\mathcal{S},\mathcal{A},\mathcal{P},r,\gamma>$

- $\mathcal{S}$ 是状态集合
- $\mathcal{A}$ 是动作集合
- $\gamma$ 是折扣因子
- $r(s,a)$ 是奖励函数，此时奖励同时取决于状态 $s$ 和动作 $a$ ，当奖励只取决于状态 $s$ 时，奖励函数退化为 $r(s)$
- $P(s'|s,a)$ 是状态转移函数，表示状态 $s$ 在执行动作 $a$ 后到达状态 $s'$ 的概率

回报 $G_t=R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+...=\sum_{k=0}^{\infty}\gamma^kR_{t+k}$ ，表示从 $t$ 时刻开始直到终止时所有奖励之和

价值 $V(s)$，表示在状态 $s$ 获得的期望回报，其贝尔曼方程为

$$
\begin{align*}
V(s)
&= \mathbb{E}[G_t|S_t=s] \\
&= \mathbb{E}[R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+...|S_t=s] \\
&= \mathbb{E}[R_t+\gamma(R_{t+1}+\gamma R_{t+2}+...)|S_t=s] \\
&= \mathbb{E}[R_t+\gamma G_{t+1}|S_t=s] \\
&= \mathbb{E}[R_t+\gamma V(S_{t+1})|S_t=s] \\
&= r(s)+\gamma \sum_{s'\in S}p(s'|s)V(s')
\end{align*}
$$

写成矩阵形式

$$V=R+\gamma PV$$

可得价值函数解析解

$$V=(I-\gamma P)^{-1}R$$

策略 $\pi(a|s)=P(A_t=a|S_t=s)$，表示在状态 $s$ 采取动作 $a$ 的概率

状态价值函数 $V^{\pi}(s)=\mathbb{E}_{\pi}[G_t|S_t=s]$，表示在状态 $s$ 遵循策略 $\pi$ 获得的期望回报，其贝尔曼方程为

$$
\begin{align*}
V^{\pi}(s)
&= \mathbb{E}[R_t+\gamma V^{\pi}(S_{t+1})|S_t=s] \\
&= \sum_{a \in A}\pi(a|s)(r(s,a)+\gamma \sum_{s'\in S}p(s'|s,a)V^{\pi}(s'))
\end{align*}
$$

动作价值函数 $Q^{\pi}(s,a)=\mathbb{E}_{\pi}[G_t|S_t=s,A_t=a]$，表示从状态 $s$ 遵循策略 $\pi$ 执行动作 $a$ 得到的期望回报

状态价值函数和动作价值函数关系

- 在使用策略 $\pi$ 中，状态 $s$ 的价值等于在该状态下基于策略 $\pi$ 采取所有动作的概率与相应的价值相乘再求和的结果

$$V^{\pi}(s)=\sum_{a \in A}\pi(a|s)Q^{\pi}(s,a)$$

- 使用策略 $\pi$ 时，状态 $s$ 下采取动作 $a$ 的价值等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积：

$$Q^{\pi}(s,a)=r(s,a)+\gamma\sum_{s'\in{S}}P(s'|s,a)V^{\pi}(s')$$

其贝尔曼期望方程为

$$
\begin{align*}
Q^{\pi}(s,a)
&= \mathbb{E}[R_t+\gamma Q^{\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a] \\
&= r(s,a) + \gamma \sum_{s'\in S}p(s'|s,a) \sum_{a' \in A}\pi(a'|s')Q^{\pi}(s',a')
\end{align*}
$$

1. 策略的状态访问分布——策略对状态 $s$ 被访问到的概率

$$ v^{\pi}(s) = (1 - \gamma) \sum\limits_{t=0}^{\infty}\gamma^tP_t^{\pi}(s) $$

其中，$P_t^{\pi}(s)$ 表示采取策略 ${\pi}$ 使得智能体在时刻 $t$ 状态为 $s$ 的概率，有 $P_0^{\pi}(s) = v_0(s)$，$v_0(s)$ 是初始状态分布

$$ v^{\pi}(s') = (1 - \gamma)v_0(s') + \gamma \int{P(s'|s,a)\pi(a|s)v^{\pi}(s)dsda} $$

2. 策略的占用度量——策略对状态动作对 $(s,a)$ 被访问到的概率

$$ \rho^{\pi}(s,a) = (1 - \gamma) \sum\limits_{t=0}^{\infty}\gamma^tP_t^{\pi}(s)\pi(a|s) $$

$$ \rho^{\pi}(s,a) = v^{\pi}(s) \pi(a|s) $$

最优策略 $ \pi^{*}(s) $

在有限状态和动作集合的 MDP 中，至少存在一个策略比其他所有策略都好或者至少存在一个策略不差于其他所有策略，这个策略就是最优策略

同理，

最优状态价值函数

$$
V^{*}(s) = \max\limits_{\pi}V^{\pi}(s)
$$

最优动作价值函数

$$
Q^{*}(s,a) = \max\limits_{\pi}Q^{\pi}(s,a)
$$

最优状态价值函数和最优动作价值函数之间的关系

$$Q^{*}(s,a)=r(s,a)+\gamma\sum_{s'\in{S}}P(s'|s,a)V^{*}(s')$$

贝尔曼最优方程

$$
\begin{align*}
V^{*}(s)
= \max_{a \in A}\pi(a|s)(r(s,a)+\gamma \sum_{s'\in S}p(s'|s,a)V^{*}(s'))
\end{align*}
$$

$$
\begin{align*}
Q^{\pi}(s,a)
= r(s,a) + \gamma \sum_{s'\in S}p(s'|s,a) \max_{a' \in A}Q^{*}(s',a')
\end{align*}
$$


MDP问题求解

1. 转换成 MRP 问题求解析解

$$
r'(s) = \sum_{a \in \mathcal{A}} \pi(a|s)r(s,a)
$$

$$
P'(s'|s) = \sum_{a \in \mathcal{A}} \pi(a|s)P(s'|s,a)
$$

$$ <\mathcal{S},\mathcal{A},\mathcal{P},r,\gamma> \rightarrow <\mathcal{S},\mathcal{P}',r',\gamma> $$

2. 蒙特卡洛方法

$$
V^{\pi}(s)=\mathbb{E}_{\pi}[G_t|S_t=s]\approx
\frac{1}{N}\sum_{i=1}^NG_t^{(i)}
$$

使用策略 $\pi$ 采样若干条序列

$$ s_0^{(i)} \stackrel{a_0^{(i)}}{\longrightarrow} r_0^{(i)}, s_1^{(i)} \stackrel{a_1^{(i)}}{\longrightarrow} r_1^{(i)}, s_2^{(i)} \stackrel{a_2^{(i)}}{\longrightarrow} \cdots \stackrel{a_{T-1}^{(i)}}{\longrightarrow} r_{T-1}^{(i)}, s_T^{(i)} $$

对每一条序列中每一时间步 $t$ 的状态 $s$ 进行

- 更新状态 $s$ 的计数器 $N(s) \leftarrow N(s) + 1$
- 更新状态 $s$ 的价值 $V(s) \leftarrow V(s) + \frac{1}{N(s)}(G-V(s))$

3. 动态规划算法
4. 时序差分算法

## 2.多臂老虎机——无状态强化学习

多臂老虎机的数学描述—— $<A,R>$

- $A$ 是动作集合，如果多臂老虎机一共有 $K$ 根拉杆，那么 $A=\{a_1,a_2,...,a_K\}$， $a_t$ 表示拉动第 $t$ 根拉杆
- $R$ 是奖励概率分布，拉动每一根拉杆的动作 $a$ 都对应一个奖励概率分布 $R(r|a)$

假设每个时间步只能拉动一次拉杆，多臂老虎机的目标为最大化一段时间内的累计奖励 $max \sum_{t=1}^T r_t$

**探索和利用的平衡**：探索是指尝试拉动更多可能的拉杆，摸清楚所有拉杆的获奖情况，利用是指拉动已知期望奖励最大的那根拉杆，由于已知的信息仅仅来自有限次的交互观测，所以获取全局最优的关键在于平衡探索和利用

### $\epsilon$-贪婪算法/ $\epsilon$-贪婪衰减算法

每次以概率 $\epsilon$ 选择以往经验中期望奖励估值最大的那根拉杆（利用），以概率 $\epsilon$ 随机选择一根拉杆（探索）

### 上置信界算法

每次选择拉杆前，先估计每根拉杆的期望奖励的上界，使得拉动每根拉杆的期望奖励只有一个较小的概率 $p$ 超过这个上界，接着选出期望奖励上界最大的拉杆，从而选择最有可能获得最大期望奖励的拉杆

### Thompson采样算法

假设拉动每根拉杆的奖励服从一个特定的概率分布，然后根据拉动每根拉杆的期望奖励来进行选择。但是由于计算所有拉杆的期望奖励的代价比较高，汤普森采样算法使用采样的方式，即根据当前每个动作$a$的奖励概率分布进行一轮采样，得到一组各根拉杆的奖励样本，再选择样本中奖励最大的动作。